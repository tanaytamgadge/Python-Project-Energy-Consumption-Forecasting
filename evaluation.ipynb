{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi7ROVMreMik"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
        "from sklearn.model_selection import cross_val_score, KFold, learning_curve, train_test_split\n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "\n",
        "# Load the saved models\n",
        "with open('linear_regression_model.pkl', 'rb') as lr_file:\n",
        "    regressor_lr = pickle.load(lr_file)\n",
        "\n",
        "with open('random_forest_model.pkl', 'rb') as rf_file:\n",
        "    regressor_rf = pickle.load(rf_file)\n",
        "\n",
        "with open('scaler.pkl', 'rb') as scaler_file:\n",
        "    scaler = pickle.load(scaler_file)\n",
        "\n",
        "# Load the preprocessed dataset (same dataset used for training and testing)\n",
        "data = pd.read_csv(\"energy_consumption_preprocessed.csv\")\n",
        "\n",
        "# Feature selection and target (same as before)\n",
        "features = ['Temperature', 'Humidity', 'Wind Speed', 'Hour', 'Day', 'Weekday', 'Month', 'Holiday_1', 'Season_Summer', 'Season_Winter']\n",
        "target = 'Consumption'\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Standardizing the features using StandardScaler (same scaler used during training)\n",
        "X_scaled = scaler.transform(X)\n",
        "\n",
        "# Define K-fold cross-validation strategy\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate performance using K-fold Cross Validation\n",
        "def evaluate_model_kfold(model, X, y, model_name=\"Model\"):\n",
        "    cv_results = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "    print(f\"\\nK-fold Cross-Validation for {model_name} Model:\")\n",
        "    print(f\"Mean MSE: {-np.mean(cv_results):.4f}\")\n",
        "    print(f\"Mean RMSE: {np.sqrt(-np.mean(cv_results)):.4f}\")\n",
        "    return -np.mean(cv_results), np.sqrt(-np.mean(cv_results))\n",
        "\n",
        "# Ridge and Lasso Regression (with hyperparameter tuning)\n",
        "def evaluate_ridge_lasso(X, y):\n",
        "    alpha_values = [0.1, 0.5, 1.0, 5.0, 10.0]  # Regularization strengths\n",
        "    for alpha in alpha_values:\n",
        "        ridge = Ridge(alpha=alpha)\n",
        "        lasso = Lasso(alpha=alpha)\n",
        "\n",
        "        # Ridge\n",
        "        print(f\"Evaluating Ridge Regression with alpha={alpha}...\")\n",
        "        ridge.fit(X, y)\n",
        "        y_pred_ridge = ridge.predict(X)\n",
        "        evaluate_model(y, y_pred_ridge, f\"Ridge (alpha={alpha})\")\n",
        "\n",
        "        # Lasso\n",
        "        print(f\"Evaluating Lasso Regression with alpha={alpha}...\")\n",
        "        lasso.fit(X, y)\n",
        "        y_pred_lasso = lasso.predict(X)\n",
        "        evaluate_model(y, y_pred_lasso, f\"Lasso (alpha={alpha})\")\n",
        "\n",
        "# Model comparison and detailed evaluation for Ridge, Lasso, Linear Regression, and Random Forest\n",
        "evaluate_ridge_lasso(X_scaled, y)\n",
        "\n",
        "# Hyperparameter tuning for Random Forest using Grid Search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def random_forest_tuning(X, y):\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "    grid_search_rf = GridSearchCV(estimator=regressor_rf, param_grid=param_grid, cv=kf, n_jobs=-1, verbose=2)\n",
        "    grid_search_rf.fit(X, y)\n",
        "\n",
        "    print(f\"Best Parameters for Random Forest: {grid_search_rf.best_params_}\")\n",
        "\n",
        "    best_rf_model = grid_search_rf.best_estimator_\n",
        "    y_pred_rf = best_rf_model.predict(X)\n",
        "    evaluate_model(y, y_pred_rf, \"Random Forest (Tuned)\")\n",
        "\n",
        "# Random Forest with Grid Search hyperparameter tuning\n",
        "random_forest_tuning(X_scaled, y)\n",
        "\n",
        "# Residual Analysis (with Quantile Residuals)\n",
        "def quantile_residuals(y_true, y_pred, quantile=0.5):\n",
        "    residuals = y_pred - y_true\n",
        "    quantile_residuals = np.maximum(quantile * residuals, (quantile - 1) * residuals)\n",
        "    return quantile_residuals\n",
        "\n",
        "# Calculate quantile residuals for Random Forest and Ridge models\n",
        "quantile_residuals_rf = quantile_residuals(y, y_pred_rf, quantile=0.5)\n",
        "quantile_residuals_ridge = quantile_residuals(y, y_pred_ridge, quantile=0.5)\n",
        "\n",
        "# Plot Quantile Residuals for Random Forest and Ridge\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(quantile_residuals_rf, kde=True, color='purple', label=\"Random Forest Quantile Residuals\")\n",
        "sns.histplot(quantile_residuals_ridge, kde=True, color='orange', label=\"Ridge Quantile Residuals\")\n",
        "plt.title('Distribution of Quantile Residuals')\n",
        "plt.xlabel('Quantile Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Learning Curves for Hyperparameter Tuning\n",
        "def plot_learning_curve(estimator, X, y, title=\"Learning Curve\", cv=None):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=-1)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Score', color='blue')\n",
        "    plt.plot(train_sizes, np.mean(test_scores, axis=1), label='Cross-validation Score', color='red')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Training Samples')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Learning Curve for Random Forest after Hyperparameter Tuning\n",
        "plot_learning_curve(grid_search_rf.best_estimator_, X_scaled, y, title=\"Learning Curve (Random Forest Tuning)\", cv=kf)\n",
        "\n",
        "# Performance by Weekday vs Weekend\n",
        "data['Weekend'] = data['Weekday'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)\n",
        "\n",
        "# Splitting data into weekday and weekend subsets\n",
        "X_weekday = data[data['Weekend'] == 0][features]\n",
        "y_weekday = data[data['Weekend'] == 0][target]\n",
        "\n",
        "X_weekend = data[data['Weekend'] == 1][features]\n",
        "y_weekend = data[data['Weekend'] == 1][target]\n",
        "\n",
        "# Evaluate performance on Weekday and Weekend separately\n",
        "print(\"\\nEvaluating Model on Weekday Data...\")\n",
        "y_pred_weekday = regressor_rf.predict(X_weekday)\n",
        "evaluate_model(y_weekday, y_pred_weekday, \"Random Forest (Weekday)\")\n",
        "\n",
        "print(\"\\nEvaluating Model on Weekend Data...\")\n",
        "y_pred_weekend = regressor_rf.predict(X_weekend)\n",
        "evaluate_model(y_weekend, y_pred_weekend, \"Random Forest (Weekend)\")\n",
        "\n",
        "# Visualizing the predictions (weekday vs weekend)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_weekday.values, label='Actual Consumption (Weekday)', color='blue')\n",
        "plt.plot(y_pred_weekday, label='Predicted Consumption (Weekday)', color='green', linestyle='dashed')\n",
        "plt.title('Actual vs Predicted Energy Consumption (Weekday)')\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Consumption (kWh)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_weekend.values, label='Actual Consumption (Weekend)', color='blue')\n",
        "plt.plot(y_pred_weekend, label='Predicted Consumption (Weekend)', color='green', linestyle='dashed')\n",
        "plt.title('Actual vs Predicted Energy Consumption (Weekend)')\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Consumption (kWh)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Saving the evaluation results to CSV (for analysis later)\n",
        "evaluation_results = pd.DataFrame({\n",
        "    'Actual Consumption': y,\n",
        "    'Predicted LR': y_pred_lr,\n",
        "    'Predicted RF': y_pred_rf,\n",
        "    'Predicted Ridge': y_pred_ridge,\n",
        "    'Predicted Lasso': y_pred_lasso\n",
        "})\n",
        "\n",
        "evaluation_results.to_csv(\"advanced_model_evaluation_results.csv\", index=False)\n",
        "\n",
        "print(\"Advanced evaluation results saved successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
